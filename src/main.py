#!/usr/bin/env python3
# coding: utf-8
# @Author: ArthurBernard
# @Email: arthur.bernard.92@gmail.com
# @Date: 2023-10-05 12:09:35
# @Last modified by: ArthurBernard
# @Last modified time: 2024-10-18 11:31:44

""" Main script to train LLM model. """

# Built-in packages
from pathlib import Path
from random import seed

# Third party packages
from torch.optim import AdamW
from transformers import TextStreamer

# Local packages
from config import LOG, ACCUMULATION_STEPS, LR, TrainingParser
from save_load import Checkpoint, LoaderLLM
from trainer import Trainer
from utils import generate, shuffle_per_batch

__all__ = []

# General parameters
# DATA_PATH = Path("./data/full_scripts.json")
DATA_PATH = Path("./data/LLM_Solutions.json")
MODEL_NAME = Path("./models/Llama-3.2-1B")
# MODEL_NAME = Path("./models/MiniChatBot-1B")

PROMPT = ("This is a conversation between User and MiniChatBot. MiniChatBot "
          "answers questions related to LLM Solutions, Arthur Bernard, and "
          "the services offered by LLM Solutions.")

# Evaluation sentences
EVAL_SENTENCES = [
    "User: What is LLM Solution ?\nMiniChatBot: ",
    "User: Who is the USA president ?\nMiniChatBot: ",
    "User: Qui est le cr√©ateur de LLM Solution ?\nMiniChatBot: ",
]


class Main(LoaderLLM):
    def __init__(
        self,
        model_name: Path,
        batch_size: int,
        data_path: Path,
        checkpoint: bool | Checkpoint,
        device: str,
        **kw_load_model,
    ):
        self.batch_size = batch_size
        self.checkpoint = checkpoint
        self.device = device
        self.name = model_name.name

        # Load tokenizer, model and data (or last available checkpoint)
        LoaderLLM.__init__(self, model_name, data_path, checkpoint,
                           **kw_load_model)

        # Process data
        self.process_data()

    def __call__(self, eval_sentences: list = None):
        # Test eval model before training
        if eval_sentences is not None:
            LOG.info("Eval test before training")
            self.eval(*eval_sentences)

        # Training
        try:
            trainer = Trainer(
                self.llm,
                self.tokenizer,
                self.data,
                self.batch_size,
                accumulation_steps=ACCUMULATION_STEPS,
            )

            trainer.set_optimizer(AdamW, self.llm.parameters(), lr=LR)

            trainer.run(device=self.device, checkpoint=self.checkpoint)

            # Save trained model
            path = Path("./models/trained") / TRAINING_TYPE / self.name
            self.checkpoint.save_trained_model(self.llm, path, self.tokenizer)

        except Exception as e:
            LOG.error(f"The following error occurs: {type(e)} - {e}")

        # Test eval model after training
        if eval_sentences is not None:
            LOG.info("Eval test after training")
            self.eval(*eval_sentences)

    def eval(self, *sentences: str, max_length: int = 32):
        """ Evaluate LLM by generate some sentences.

        Parameters
        ----------
        *sentences : str
            Sentence to evaluate the LLM.
        max_length : int, optional
            Maximum number of tokens generated by the LLM, default is 32
            tokens.

        """
        for sentence in sentences:
            length = len(self.tokenizer(sentence).input_ids)

            # ans = generate(
            #     self.llm,
            #     self.tokenizer,
            #     sentence,
            #     max_length=length + max_length,
            #     device=self.device,
            # )
            # LOG.info(ans)
            s = f"{PROMPT}\n\n{sentence}"
            length = len(self.tokenizer(s).input_ids)

            encoded = self.tokenizer(s, return_tensors='pt')
            streamer = TextStreamer(self.tokenizer)
            _ = self.llm.generate(
                **encoded,
                max_length=length + max_length,
                streamer=streamer,
            )
            LOG.info(self.tokenizer.decode(_))

    def print_trainable_parameters(self):
        """ Display trainable parameters. """
        trainable_params = 0
        all_param = 0

        for _, param in self.llm.named_parameters():
            all_param += param.numel()

            if param.requires_grad:
                trainable_params += param.numel()

        LOG.info(f"\n\nTrainable params: {trainable_params:,} || All params: "
                 f"{all_param:,} || Trainable: "
                 f"{trainable_params / all_param:.2%}\n\n")

    def process_data(self):
        """ Process and shuffle data. """
        # Data from json to string
        self.data = [
            f"{PROMPT}\n\nUser: {d['User']}\nMiniChatBot: {d['MiniChatBot']}"
            for d in self.data
        ]

        # Shuffle data
        seed(42)
        self.data = shuffle_per_batch(
            self.data,
            self.tokenizer,
            batch_size=self.batch_size,
        )


if __name__ == "__main__":
    import logging.config
    import yaml

    # Load logging configuration
    with open('./logging.ini', 'r') as f:
        log_config = yaml.safe_load(f.read())

    logging.config.dictConfig(log_config)

    # Get training arguments
    parser = TrainingParser(file=__file__)
    args = parser()
    LOG.info(f"{parser}\n")

    if args.checkpoint:
        checkpoint = Checkpoint(
            path=args.checkpoint_path,
            timestep=args.checkpoint_timestep,
        )

    else:
        checkpoint = False

    main = Main(args.model, args.batch_size, args.data_path, checkpoint,
                args.device)
    main(EVAL_SENTENCES)

    # Test
    # from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

    # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

    # for sentence in EVAL_SENTENCES:
    #     s = f"{PROMPT}\n\n{sentence}"
    #     length = len(tokenizer(s).input_ids)

    #     encoded = tokenizer(s, return_tensors='pt')
    #     streamer = TextStreamer(tokenizer)
    #     _ = model.generate(**encoded, max_length=length + 32, streamer=streamer)
